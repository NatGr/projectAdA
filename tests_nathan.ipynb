{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "import seaborn as sns\n",
    "sns.set_context(\"notebook\")\n",
    "import plotly.plotly as py\n",
    "from scipy.stats import norm\n",
    "from bokeh.plotting import figure, show\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy.stats import norm\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "DATA_DIR = 'data'\n",
    "MIN_CONSIDER = 2000  # minimum number of datapoints to consider the data\n",
    "MIN_CI = 10000  # minimum number of datapoints under which we assume the normal mean assumption holds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_to_country_view = pd.read_csv(os.path.join(DATA_DIR, 'country_to_country_view.csv')).dropna()  # 3 rows have no mention_country\n",
    "country_inner_view = pd.read_csv(os.path.join(DATA_DIR, 'country_inner_view.csv'))\n",
    "country_outer_view = pd.read_csv(os.path.join(DATA_DIR, 'country_outer_view.csv'))\n",
    "country_inner_type_view = pd.read_csv(os.path.join(DATA_DIR, 'country_inner_type_view.csv'))\n",
    "country_outer_type_view = pd.read_csv(os.path.join(DATA_DIR, 'country_outer_type_view.csv'))\n",
    "country_to_type_view = pd.read_csv(os.path.join(DATA_DIR, 'country_to_country_type_view.csv'))\n",
    "media_to_country_view = pd.read_csv(os.path.join(DATA_DIR, 'media_to_country_view.csv.zip'))\n",
    "\n",
    "## we can already sort out datum with few mentions in inner and outer views since we won't regroup them\n",
    "country_inner_view = country_inner_view[country_inner_view.count_mentions > MIN_CONSIDER]\n",
    "country_outer_view = country_outer_view[country_outer_view.count_mentions > MIN_CONSIDER]\n",
    "country_inner_type_view = country_inner_type_view[country_inner_type_view.count_mentions > MIN_CONSIDER]\n",
    "country_outer_type_view = country_outer_type_view[country_outer_type_view.count_mentions > MIN_CONSIDER]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_codes = pd.read_csv(os.path.join(DATA_DIR, 'mapping_country_codes.csv')).drop(\n",
    "    \"Unnamed: 0\", axis=1)  # needed for plotly\n",
    "country_codes.columns = [\"country_name\", \"country_ISO\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from https://github.com/scikit-learn/scikit-learn/blob/70cf4a676caa2d2dad2e3f6e4478d64bcb0506f7/examples/cluster/plot_hierarchical_clustering_dendrogram.py\n",
    "def plot_dendrogram(model, **kwargs):\n",
    "    \"\"\"\n",
    "    =========================================\n",
    "    Plot Hierarachical Clustering Dendrogram \n",
    "    =========================================\n",
    "    This example plots the corresponding dendrogram of a hierarchical clustering\n",
    "    using AgglomerativeClustering and the dendrogram method available in scipy.\n",
    "    \"\"\"\n",
    "\n",
    "    # Children of hierarchical clustering\n",
    "    children = model.children_\n",
    "\n",
    "    # Distances between each pair of children\n",
    "    # Since we don't have this information, we can use a uniform one for plotting\n",
    "    distance = np.arange(children.shape[0])\n",
    "\n",
    "    # The number of observations contained in each cluster level\n",
    "    no_of_observations = np.arange(2, children.shape[0]+2)\n",
    "\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "    linkage_matrix = np.column_stack([children, distance, no_of_observations]).astype(float)\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    dendrogram(linkage_matrix, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_CI_to_DF(df, mean_col_name=\"avg_tone\", std_col_name=\"std_tone\", count_col_name=\"count_mentions\", std_mean_col_name=None, alpha=0.05):\n",
    "    \"\"\"\n",
    "    by the Continuous limit theorem, the mean of a R.V. is distributed as N(\\mu, \\sigma^2/n)\n",
    "    We will compute the bounds and add them to the dataframe assuming the CLT holds even tough \n",
    "    n is finite\n",
    "    \n",
    "    :param df: the dataframe\n",
    "    :param mean_col_name: the name of the column of df containing the mean of the data\n",
    "    :param std_col_name: the name of the column of df containing the std of the data\n",
    "    :param count_col_name: the name of the column of df containing the count of the data\n",
    "    :param std_mean_col_name: the std of the mean (if this is set, std_col_name and count_col_name are not used)\n",
    "    :param alpha: the parameter alpha corresponding to the CI (ex: 0.05 for 95% CI)\n",
    "    \"\"\"\n",
    "\n",
    "    interval = norm.interval(1-alpha)\n",
    "    if std_mean_col_name is None:\n",
    "        mean_std = df[std_col_name] / (df[count_col_name]**0.5)\n",
    "    else:\n",
    "        mean_std = df[std_mean_col_name]\n",
    "    df[mean_col_name + \"_low_CI\"] = interval[0]*mean_std + df[mean_col_name]\n",
    "    df[mean_col_name + \"_high_CI\"] = interval[1]*mean_std + df[mean_col_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_plotly(df, data_col_name, file_name, country_codes=country_codes, \n",
    "                     country_col_name=\"country\"):\n",
    "    \"\"\"\n",
    "    generates a plotly graph from the given dataframe\n",
    "    \n",
    "    :param dfs: a dataframe\n",
    "    :param data_col_name: the name of the column containing the data\n",
    "    :param file_name: name of the file\n",
    "    :param country_codes: the df that contains the mapping from country names to country codes\n",
    "    :param country_col_name: the name of the column of df that contains the country names\n",
    "    \"\"\"\n",
    "    output = df.merge(country_codes, left_on=country_col_name, right_on=\"country_name\",\n",
    "                          suffixes=('',''))\n",
    "    output[\"text\"] = output.apply(lambda x: f\"{x.country} <br>\" + \n",
    "    (f\"95% CI: [{x[data_col_name + '_low_CI'] :0.3f}, {x[data_col_name + '_high_CI']: 0.3f}] <br>\"\n",
    "    if x.count_mentions > MIN_CI else \"\") + f\"{x.count_mentions: .0E} datapoints\", axis=1)\n",
    "    data = [ dict(\n",
    "            type = 'choropleth',\n",
    "            locations = output.country_ISO,\n",
    "            z = np.round(output[data_col_name], 3),\n",
    "            text = output.text,\n",
    "            autocolorscale = True,\n",
    "            reversescale = True,\n",
    "            colorbar = dict(\n",
    "                autotick = False,\n",
    "                title = 'TODO'),\n",
    "          ) ]\n",
    "\n",
    "    layout = dict(\n",
    "        title = 'TODO',\n",
    "        geo = dict(\n",
    "            showframe = False,\n",
    "            showcoastlines = True,\n",
    "            projection = dict(\n",
    "                type = 'Mercator'\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig = dict(data=data, layout=layout)\n",
    "    py.iplot(fig, validate=False, filename=file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Countries ranking\n",
    "First, we simply save the outer views of each country. We then try to take into account the score of a country to weight the tone he uses to talk about other countries, unfortunately, most of the countries scores are negative, which means that a country referenced by a lot of other countries in a negative way will be positively impacted (e.g. USA). A solution to this problem would be to change the avg_tones (uniformy while modifying them as little as possible) so that all the countries are positively coted and to sort that data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_CI_to_DF(country_outer_view)\n",
    "# export_to_plotly(country_outer_view, \"avg_tone\", \"outer_view_base_plot\")  # uncomment to export to plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decreasing the weight of a country with bad reputation to compute the reputation of another \n",
    "# country\n",
    "c_to_c_transition = country_to_country_view[[\"actor_country\", \"mention_country\", \"avg_tone\", \n",
    "                                              \"count_mentions\", \"avg_weighted_tone\"]]\n",
    "# other columns cannot be splitted from country to country\n",
    "\n",
    "c_to_c_transition = c_to_c_transition[c_to_c_transition.actor_country !=\n",
    "                                             c_to_c_transition.mention_country]\n",
    "\n",
    "\n",
    "# get the count of articles for each actor_country and weight avg_tone and avg_weighted_tone by \n",
    "# this, i.e. avg_tone = avg_tone * count / (sum count for actor_country)\n",
    "counts = c_to_c_transition.groupby(\"actor_country\")[\"count_mentions\"].agg(\"sum\")\n",
    "c_to_c_transition = c_to_c_transition.merge(counts.to_frame(), left_on=\"actor_country\",\n",
    "                                            right_on=\"actor_country\")\n",
    "c_to_c_transition.avg_tone = (c_to_c_transition.avg_tone + 7)* c_to_c_transition.count_mentions_x / \\\n",
    "                            c_to_c_transition.count_mentions_y\n",
    "# the value of 7 is chosen (by trial and error) so that it is as small as possible (the bigger \n",
    "# this value, the less tones are different and the the more \"uniform\" the data becomes) and that\n",
    "# there are no countries with a negative score (being criticized by these countries would mean\n",
    "# we are good and initially, nearly all countries were negatives)\n",
    "c_to_c_transition = c_to_c_transition.drop([\"count_mentions_x\", \"count_mentions_y\"], axis=1)\n",
    "\n",
    "\n",
    "# putting the country to country transitions in the form of a np matrix (graph transition matrix\n",
    "# ) instead of using the power method, we will compute teh eigenvalues/eigenvectors by\n",
    "# using numpy \"eig\" function\n",
    "index_to_country = np.unique(country_to_country_view.actor_country.values)\n",
    "country_to_index = {country: index for index, country in enumerate(index_to_country)}\n",
    "\n",
    "transition_matrix = np.zeros((index_to_country.shape[0], index_to_country.shape[0]))\n",
    "for row in c_to_c_transition.itertuples(index=False):\n",
    "    if row.mention_country in country_to_index:  # we do not take the country that don't have\n",
    "        # mentions from other countries into account\n",
    "        transition_matrix[country_to_index[row.actor_country],\n",
    "                      country_to_index[row.mention_country]] = row.avg_tone\n",
    "\n",
    "eig_val, eig_vec = np.linalg.eig(transition_matrix)\n",
    "\n",
    "# get the biggest read eigenvalue associated with a read eigenvector\n",
    "eig_vec = eig_vec[:, np.isreal(eig_val)]\n",
    "eig_val = eig_val[np.isreal(eig_val)].real\n",
    "real_eig_vec = np.sum(np.iscomplex(eig_vec), 0) == 0  # real eigenvectors\n",
    "eig_val = eig_val[real_eig_vec]\n",
    "eig_vec = eig_vec[:, real_eig_vec].real\n",
    "\n",
    "max_eig_val_idx = np.argmax(eig_val)\n",
    "eig_vec = eig_vec[:, max_eig_val_idx]\n",
    "eig_val = eig_val[max_eig_val_idx]\n",
    "\n",
    "country_view = pd.DataFrame(data=eig_vec, index=index_to_country, columns=[\"avg_tone_graph\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare both rankings, we only compute the ranking for the 40 biggest countries\n",
    "country_view = country_view.merge(country_outer_view[country_outer_view.count_mentions > 1e6], left_index=True, right_on=\"country\")\n",
    "country_view = country_view[[\"country\", \"avg_tone\", \"avg_tone_graph\"]]\n",
    "\n",
    "# we will use the indexes to get the rankings\n",
    "country_view = country_view.sort_values(by=\"avg_tone\", ascending=False).reset_index()\n",
    "country_view[\"normal_ranking\"] = country_view.index + 1\n",
    "country_view = country_view.sort_values(by=\"avg_tone_graph\", ascending=False).reset_index()\n",
    "country_view[\"graph_ranking\"] = country_view.index + 1\n",
    "country_view[\"places_won_with_graph\"] = country_view[\"normal_ranking\"] - country_view[\"graph_ranking\"]\n",
    "country_view = country_view[[\"country\", \"normal_ranking\", \"graph_ranking\", \"places_won_with_graph\"]]\n",
    "country_view = country_view.sort_values(by=\"places_won_with_graph\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_view.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_view.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inner - Outer View\n",
    "computes the difference between the inner and outer view for each country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_keep = [\"country\", \"avg_tone\", \"std_tone\", \"count_mentions\"]\n",
    "diff_in_out = country_inner_view[cols_to_keep].merge(country_outer_view[cols_to_keep], \n",
    "    right_on=\"country\", left_on=\"country\")\n",
    "diff_in_out[\"difference\"] = diff_in_out[\"avg_tone_x\"] - diff_in_out[\"avg_tone_y\"]\n",
    "diff_in_out[\"count_mentions\"] = np.minimum(diff_in_out.count_mentions_x, \n",
    "                diff_in_out.count_mentions_y)  # we take the minimum between inner and\n",
    "# outer count_mentions as number of count_mentions for the difference\n",
    "\n",
    "# compute the confidence intervals\n",
    "# std_mean(inner-outer) = sqrt(std_mean(inner)^2 + std_mean(outer)^2) -- assuming independence \n",
    "diff_in_out[\"std_diff\"] = (diff_in_out.std_tone_x**2 / diff_in_out.count_mentions_x + \\\n",
    "                          diff_in_out.std_tone_y**2 / diff_in_out.count_mentions_y) ** 0.5\n",
    "add_CI_to_DF(diff_in_out, mean_col_name=\"difference\", std_mean_col_name=\"std_diff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export_to_plotly(diff_in_out, \"difference\", \"difference_plot\")  # uncomment to export to plotly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyzing the influence of actor types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_types = [\"COP\", \"EDU\", \"GOV\", \"JUD\", \"LEG\", \"MED\", \"MNC\", \"MIL\"]\n",
    "actor_types_description = {\"COP\": \"Police forces\", \"GOV\": \"Government\", \"EDU\": \"Education\",\n",
    "                          \"JUD\": \"Judiciary\", \"LEG\": \"Legislature\", \"MED\": \"Media\",\n",
    "                           \"MNC\": \"Multinational corporation\", \"MIL\": \"Military\"}\n",
    "country_outer_type_view = country_outer_type_view[country_outer_type_view.actor_type.apply(\n",
    "    lambda x: x in actor_types)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_outer_type_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "c_to_t_selected_type = country_to_type_view[country_to_type_view.actor_type == selected_type]\n",
    "select_out = c_to_t_selected_type.apply(lambda x: x.actor_country != x.mention_country, axis=1)\n",
    "outer_view_type = c_to_t_selected_type[select_out].copy()\n",
    "outer_view_type[\"sum_mentions\"] = outer_view_type.groupby(\"actor_country\").count_mentions.transform(\"sum\")\n",
    "outer_view_type = outer_view_type.groupby(\"actor_country\").apply(lambda x: x.avg_tone * x.count_mentions \\\n",
    "                                        / x.sum_mentions).groupby(\"actor_country\").sum()\n",
    "\n",
    "inner_view_type = c_to_t_selected_type[np.logical_not(select_out)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_view_type_rel = inner_view_type[inner_view_type[\"count_events\"] > 1000]\n",
    "outer_view_type_rel = pd.DataFrame(outer_view_type, columns=[\"avg_tone\"])\n",
    "inner_outer_view = inner_view_type_rel[[\"actor_country\", \"avg_tone\"]].merge(outer_view_type_rel, left_on=\"actor_country\", \n",
    "                                            right_index=True, suffixes=(\"_in\", \"_out\"))\n",
    "inner_outer_view.reset_index(inplace=True)\n",
    "fig, ax = plt.subplots(figsize=(16,5))\n",
    "ax.scatter(x='avg_tone_in', y='avg_tone_out', data=inner_outer_view)\n",
    "ax.set_title(\"inner vs outer view\", fontsize=15, fontweight='bold')\n",
    "ax.set_ylabel('outer_view')\n",
    "ax.set_xlabel('inner_view')\n",
    "ax.grid(True)\n",
    "\n",
    "for i, txt in enumerate(inner_outer_view.actor_country):\n",
    "    ax.annotate(txt, (inner_outer_view.avg_tone_in[i], inner_outer_view.avg_tone_out[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### clustering with or without actor_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking the biggest countries in term of references\n",
    "selected_type = \"GOV\"\n",
    "big_countries = list(country_outer_view.sort_values(by=\"count_mentions\", \n",
    "                                                    ascending=False).head(40).country)\n",
    "# only uses mentions where actor_type is selected_type\n",
    "#to_cluster = country_to_type_view[country_to_type_view.apply(lambda x: x.actor_country in big_countries \n",
    "#            and x.mention_country in big_countries and x.actor_type == selected_type , axis = 1)].copy()\n",
    "\n",
    "# all actor types\n",
    "to_cluster = country_to_country_view[country_to_country_view.apply(lambda x: x.actor_country in big_countries \n",
    "                                         and x.mention_country in big_countries, axis = 1)].copy()\n",
    "\n",
    "# proportion of time a mention from mention_country is about actorcountry\n",
    "to_cluster[\"prop_mentions\"] = to_cluster[\"count_mentions\"] / to_cluster.groupby(\"mention_country\").count_mentions.transform(\"sum\")\n",
    "\n",
    "to_cluster = to_cluster[[\"actor_country\", \"mention_country\", \"avg_tone\", \"prop_mentions\"]]\n",
    "countries = np.intersect1d(to_cluster.actor_country.unique(), to_cluster.mention_country.unique())\n",
    "to_cluster = to_cluster.set_index([\"actor_country\", \"mention_country\"])  # so that access is fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinions = np.zeros((countries.shape[0], countries.shape[0]))  # if there are no mentions about\n",
    "# that country, the count should be of 0\n",
    "for i in range(opinions.shape[0]):\n",
    "    for j in range(opinions.shape[0]):\n",
    "        try:\n",
    "            #if countries[i] in [\"United States\"]:\n",
    "                elem = to_cluster.loc[countries[i], countries[j]]\n",
    "                opinions[j,i] = elem[\"avg_tone\"] #* elem[\"prop_mentions\"] * 1000  # the 1000 is just\n",
    "                # here to avoid having too small numbers for readability of the dataframes\n",
    "                # opinions[j,i] because we categorize each country by the way he sees the other\n",
    "                # ones\n",
    "        except KeyError as e:\n",
    "            pass  # in this case we leave the default value in the array since there is no mention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinions = opinions[:, np.sum(opinions, 0) != 0]\n",
    "model = AgglomerativeClustering(n_clusters=13, affinity=\"l1\", linkage=\"average\")\n",
    "clusters = model.fit_predict(opinions)\n",
    "plot_dendrogram(model, labels=model.labels_)\n",
    "\n",
    "for i in range(np.max(clusters)+1):\n",
    "    print(countries[clusters == i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### use the different types of actors of a country to compute several ranks and compare these between countries with confidence intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_countries_outer_view = country_outer_view[country_outer_view.apply(lambda x:\n",
    "            x.country in big_countries and x.count_mentions > 10000, axis=1)].copy()\n",
    "add_CI_to_DF(big_countries_outer_view)\n",
    "\n",
    "big_countries_inner_view = country_inner_view[country_inner_view.apply(lambda x:\n",
    "            x.country in big_countries and x.count_mentions > 10000, axis=1)].copy()\n",
    "add_CI_to_DF(country_inner_view)\n",
    "\n",
    "\n",
    "actor_types = [\"COP\", \"EDU\", \"GOV\", \"JUD\", \"LEG\", \"MED\", \"MNC\", \"MIL\"]\n",
    "\n",
    "big_countries_outer_type_view = country_outer_type_view[country_outer_type_view.apply(lambda x:\n",
    " x.country in big_countries and x.actor_type in actor_types and x.count_mentions > 10000, axis=1)].copy()\n",
    "add_CI_to_DF(big_countries_outer_type_view)\n",
    "\n",
    "big_countries_inner_type_view = country_inner_type_view[country_inner_type_view.apply(lambda x:\n",
    " x.country in big_countries and x.actor_type in actor_types and x.count_mentions > 10000, axis=1)].copy()\n",
    "add_CI_to_DF(big_countries_inner_type_view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_countries_outer_view[[\"country\", \"avg_tone\", \"avg_tone_low_CI\", \"avg_tone_high_CI\"]].sort_values(\"avg_tone\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_countries_inner_type_view[big_countries_inner_type_view.actor_type == \"COP\"]\\\n",
    "[[\"country\", \"avg_tone\", \"avg_tone_low_CI\", \"avg_tone_high_CI\"]].sort_values(\n",
    "    \"avg_tone\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inner media variance\n",
    "We will sort sources of each country by importance (growing number of news) and group progressively the consecutive sources to form groups that have the same count of mentions but that contains new of very different sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_source_importance(ser, nbr_clutsers):\n",
    "    \"\"\"\n",
    "    given a serie containing the news sources of a country sorted by number of mentions and\n",
    "    their number of mentions,\n",
    "    return a label(0 to nbr_clutsers-1) for each of these sources so that each label contains\n",
    "    as much mentions and that a smaller label contains smaller sources\n",
    "    \"\"\"\n",
    "    cumsum = ser.cumsum()\n",
    "    total_nbr_mentions = cumsum.tail(1)\n",
    "    offsets = np.array([i * total_nbr_mentions/nbr_clutsers for i in range(nbr_clutsers)]).flatten()\n",
    "    # lower bound for each cluster\n",
    "    return cumsum.apply(lambda x: np.searchsorted(offsets, x) - 1) # offset in list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_cluster = media_to_country_view.groupby([\"source_country\",\n",
    "                                            \"source_name\"]).agg({\"count_mentions\": \"sum\"})\n",
    "source_cluster = source_cluster.sort_values([\"source_country\", \"count_mentions\"])\n",
    "source_cluster[\"cluster\"] = source_cluster.groupby(\"source_country\").count_mentions.apply(\n",
    "    lambda x: get_source_importance(x, 2))\n",
    "source_cluster = source_cluster.reset_index()[[\"source_name\", \"cluster\"]]\n",
    "\n",
    "news_to_country = media_to_country_view.merge(source_cluster, left_on=\"source_name\",\n",
    "                                        right_on=\"source_name\", suffixes=('',''))\n",
    "news_to_country = news_to_country.groupby([\"actor_country\", \"source_country\", \n",
    "            \"cluster\"]).agg({\"avg_tone\": \"mean\", \"count_mentions\": \"sum\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_to_country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
